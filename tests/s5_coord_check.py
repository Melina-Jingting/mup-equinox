from mup_equinox import (
    TrainingConfig, 
    CoordinateCheckConfig, 
    ModelFactory, 
    OptimizerFactory,
    CoordinateCheckRunner)
import equinox as eqx
import optax
from typing import Iterator, TypedDict
from jaxtyping import Array, Float, Integer
import jax.random as jr
import jax.numpy as jnp
import jax

from typing import List

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
from jax.nn.initializers import lecun_normal, normal
from jax.scipy.linalg import block_diag


#### Model
class GLU(eqx.Module):
    w1: eqx.nn.Linear
    w2: eqx.nn.Linear

    def __init__(self, input_dim, output_dim, key):
        w1_key, w2_key = jr.split(key, 2)
        self.w1 = eqx.nn.Linear(input_dim, output_dim, use_bias=True, key=w1_key)
        self.w2 = eqx.nn.Linear(input_dim, output_dim, use_bias=True, key=w2_key)

    def __call__(self, x):
        return self.w1(x) * jax.nn.sigmoid(self.w2(x))
    
    def get_activations(self, x):
        return {
            "w1_out": self.w1(x),
            "w2_out": self.w2(x),
            "glu_out": self.__call__(x)
        }


def make_HiPPO(N):
    """Create a HiPPO-LegS matrix.
    From https://github.com/srush/annotated-s4/blob/main/s4/s4.py
    Args:
        N (int32): state size
    Returns:
        N x N HiPPO LegS matrix
    """
    P = jnp.sqrt(1 + 2 * jnp.arange(N))
    A = P[:, jnp.newaxis] * P[jnp.newaxis, :]
    A = jnp.tril(A) - jnp.diag(jnp.arange(N))
    return -A


def make_NPLR_HiPPO(N):
    """
    Makes components needed for NPLR representation of HiPPO-LegS
     From https://github.com/srush/annotated-s4/blob/main/s4/s4.py
    Args:
        N (int32): state size

    Returns:
        N x N HiPPO LegS matrix, low-rank factor P, HiPPO input matrix B

    """
    # Make -HiPPO
    hippo = make_HiPPO(N)

    # Add in a rank 1 term. Makes it Normal.
    P = jnp.sqrt(jnp.arange(N) + 0.5)

    # HiPPO also specifies the B matrix
    B = jnp.sqrt(2 * jnp.arange(N) + 1.0)
    return hippo, P, B


def make_DPLR_HiPPO(N):
    """
    Makes components needed for DPLR representation of HiPPO-LegS
     From https://github.com/srush/annotated-s4/blob/main/s4/s4.py
    Note, we will only use the diagonal part
    Args:
        N:

    Returns:
        eigenvalues Lambda, low-rank term P, conjugated HiPPO input matrix B,
        eigenvectors V, HiPPO B pre-conjugation

    """
    A, P, B = make_NPLR_HiPPO(N)

    S = A + P[:, jnp.newaxis] * P[jnp.newaxis, :]

    S_diag = jnp.diagonal(S)
    Lambda_real = jnp.mean(S_diag) * jnp.ones_like(S_diag)

    # Diagonalize S to V \Lambda V^*
    Lambda_imag, V = jnp.linalg.eigh(S * -1j)

    P = V.conj().T @ P
    B_orig = B
    B = V.conj().T @ B
    return Lambda_real + 1j * Lambda_imag, P, B, V, B_orig


def log_step_initializer(dt_min=0.001, dt_max=0.1):
    """Initialize the learnable timescale Delta by sampling
    uniformly between dt_min and dt_max.
    Args:
        dt_min (float32): minimum value
        dt_max (float32): maximum value
    Returns:
        init function
    """

    def init(key, shape):
        """Init function
        Args:
            key: jax jr key
            shape tuple: desired shape
        Returns:
            sampled log_step (float32)
        """
        return jr.uniform(key, shape) * (jnp.log(dt_max) - jnp.log(dt_min)) + jnp.log(
            dt_min
        )

    return init


def init_log_steps(key, input):
    """Initialize an array of learnable timescale parameters
    Args:
        key: jax jr key
        input: tuple containing the array shape H and
               dt_min and dt_max
    Returns:
        initialized array of timescales (float32): (H,)
    """
    H, dt_min, dt_max = input
    log_steps = []
    for i in range(H):
        key, skey = jr.split(key)
        log_step = log_step_initializer(dt_min=dt_min, dt_max=dt_max)(skey, shape=(1,))
        log_steps.append(log_step)

    return jnp.array(log_steps)


def init_VinvB(init_fun, rng, shape, Vinv):
    """Initialize B_tilde=V^{-1}B. First samples B. Then compute V^{-1}B.
    Note we will parameterize this with two different matrices for complex
    numbers.
     Args:
         init_fun:  the initialization function to use, e.g. lecun_normal()
         rng:       jax jr key to be used with init function.
         shape (tuple): desired shape  (P,H)
         Vinv: (complex64)     the inverse eigenvectors used for initialization
     Returns:
         B_tilde (complex64) of shape (P,H,2)
    """
    B = init_fun(rng, shape)
    VinvB = Vinv @ B
    VinvB_real = VinvB.real
    VinvB_imag = VinvB.imag
    return jnp.concatenate((VinvB_real[..., None], VinvB_imag[..., None]), axis=-1)


def trunc_standard_normal(key, shape):
    """Sample C with a truncated normal distribution with standard deviation 1.
    Args:
        key: jax jr key
        shape (tuple): desired shape, of length 3, (H,P,_)
    Returns:
        sampled C matrix (float32) of shape (H,P,2) (for complex parameterization)
    """
    H, P, _ = shape
    Cs = []
    for i in range(H):
        key, skey = jr.split(key)
        C = lecun_normal()(skey, shape=(1, P, 2))
        Cs.append(C)
    return jnp.array(Cs)[:, 0]


def init_CV(init_fun, rng, shape, V):
    """Initialize C_tilde=CV. First sample C. Then compute CV.
    Note we will parameterize this with two different matrices for complex
    numbers.
     Args:
         init_fun:  the initialization function to use, e.g. lecun_normal()
         rng:       jax jr key to be used with init function.
         shape (tuple): desired shape  (H,P)
         V: (complex64)     the eigenvectors used for initialization
     Returns:
         C_tilde (complex64) of shape (H,P,2)
    """
    C_ = init_fun(rng, shape)
    C = C_[..., 0] + 1j * C_[..., 1]
    CV = C @ V
    CV_real = CV.real
    CV_imag = CV.imag
    return jnp.concatenate((CV_real[..., None], CV_imag[..., None]), axis=-1)


# Discretization functions
def discretize_bilinear(Lambda, B_tilde, Delta):
    """Discretize a diagonalized, continuous-time linear SSM
    using bilinear transform method.
    Args:
        Lambda (complex64): diagonal state matrix              (P,)
        B_tilde (complex64): input matrix                      (P, H)
        Delta (float32): discretisation step sizes             (P,)
    Returns:
        discretized Lambda_bar (complex64), B_bar (complex64)  (P,), (P,H)
    """
    Identity = jnp.ones(Lambda.shape[0])

    BL = 1 / (Identity - (Delta / 2.0) * Lambda)
    Lambda_bar = BL * (Identity + (Delta / 2.0) * Lambda)
    B_bar = (BL * Delta)[..., None] * B_tilde
    return Lambda_bar, B_bar


def discretize_zoh(Lambda, B_tilde, Delta):
    """Discretize a diagonalized, continuous-time linear SSM
    using zero-order hold method.
    Args:
        Lambda (complex64): diagonal state matrix              (P,)
        B_tilde (complex64): input matrix                      (P, H)
        Delta (float32): discretisation step sizes             (P,)
    Returns:
        discretized Lambda_bar (complex64), B_bar (complex64)  (P,), (P,H)
    """
    Identity = jnp.ones(Lambda.shape[0])
    Lambda_bar = jnp.exp(Lambda * Delta)
    B_bar = (1 / Lambda * (Lambda_bar - Identity))[..., None] * B_tilde
    return Lambda_bar, B_bar


# Parallel scan operations
@jax.vmap
def binary_operator(q_i, q_j):
    """Binary operator for parallel scan of linear recurrence. Assumes a diagonal matrix A.
    Args:
        q_i: tuple containing A_i and Bu_i at position i       (P,), (P,)
        q_j: tuple containing A_j and Bu_j at position j       (P,), (P,)
    Returns:
        new element ( A_out, Bu_out )
    """
    A_i, b_i = q_i
    A_j, b_j = q_j
    return A_j * A_i, A_j * b_i + b_j


def apply_ssm(Lambda_bar, B_bar, C_tilde, input_sequence, conj_sym):
    """Compute the LxH output of discretized SSM given an LxH input.
    Args:
        Lambda_bar (complex64): discretized diagonal state matrix    (P,)
        B_bar      (complex64): discretized input matrix             (P, H)
        C_tilde    (complex64): output matrix                        (H, P)
        input_sequence (float32): input sequence of features         (L, H)
        conj_sym (bool):         whether conjugate symmetry is enforced
    Returns:
        ys (float32): the SSM outputs (S5 layer preactivations)      (L, H)
    """
    Lambda_elements = Lambda_bar * jnp.ones(
        (input_sequence.shape[0], Lambda_bar.shape[0])
    )
    Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence)

    _, xs = jax.lax.associative_scan(binary_operator, (Lambda_elements, Bu_elements))

    if conj_sym:
        return jax.vmap(lambda x: 2 * (C_tilde @ x).real)(xs)
    else:
        return jax.vmap(lambda x: (C_tilde @ x).real)(xs)

def apply_ssm_with_activations(Lambda_bar, B_bar, C_tilde, input_sequence, conj_sym):
    """Compute the LxH output of discretized SSM given an LxH input.
    Args:
        Lambda_bar (complex64): discretized diagonal state matrix    (P,)
        B_bar      (complex64): discretized input matrix             (P, H)
        C_tilde    (complex64): output matrix                        (H, P)
        input_sequence (float32): input sequence of features         (L, H)
        conj_sym (bool):         whether conjugate symmetry is enforced
    Returns:
        ys (float32): the SSM outputs (S5 layer preactivations)      (L, H)
    """
    Lambda_elements = Lambda_bar * jnp.ones(
        (input_sequence.shape[0], Lambda_bar.shape[0])
    )
    Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence)

    _, xs = jax.lax.associative_scan(binary_operator, (Lambda_elements, Bu_elements))

    if conj_sym:
        return jax.vmap(lambda x: 2 * (C_tilde @ x).real)(xs), xs 
    else:
        return jax.vmap(lambda x: (C_tilde @ x).real)(xs), xs

class S5Layer(eqx.Module):
    Lambda_re: jax.Array
    Lambda_im: jax.Array
    B: jax.Array
    C: jax.Array
    D: jax.Array
    log_step: jax.Array

    H: int = eqx.field(static=True)
    P: int = eqx.field(static=True)
    conj_sym: bool = eqx.field(static=True)
    clip_eigs: bool = eqx.field(static=True)
    discretisation: str = eqx.field(static=True)
    step_rescale: float = eqx.field(static=True)

    def __init__(
        self,
        ssm_size,
        blocks,
        H,
        C_init: str = "trunc_standard_normal",
        conj_sym: bool = True,
        clip_eigs: bool = False,
        discretisation: str = "zoh",
        dt_min: float = 0.001,
        dt_max: float = 0.1,
        step_rescale: float = 1.0,
        *,
        key
    ):

        B_key, C_key, D_key, step_key, key = jr.split(key, 5)

        block_size = int(ssm_size / blocks)
        # Initialize state matrix A using approximation to HiPPO-LegS matrix
        Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)

        if conj_sym:
            block_size = block_size // 2
            P = ssm_size // 2
        else:
            P = ssm_size

        Lambda = Lambda[:block_size]
        V = V[:, :block_size]
        Vc = V.conj().T

        # If initializing state matrix A as block-diagonal, put HiPPO approximation
        # on each block
        Lambda = (Lambda * jnp.ones((blocks, block_size))).ravel()
        V = block_diag(*([V] * blocks))
        Vinv = block_diag(*([Vc] * blocks))

        self.H = H
        self.P = P
        if conj_sym:
            local_P = 2 * P
        else:
            local_P = P

        self.Lambda_re = Lambda.real
        self.Lambda_im = Lambda.imag

        self.conj_sym = conj_sym

        self.clip_eigs = clip_eigs

        self.B = init_VinvB(lecun_normal(), B_key, (local_P, self.H), Vinv)

        # Initialize state to output (C) matrix
        if C_init in ["trunc_standard_normal"]:
            C_init = trunc_standard_normal
        elif C_init in ["lecun_normal"]:
            C_init = lecun_normal()
        elif C_init in ["complex_normal"]:
            C_init = normal(stddev=0.5**0.5)
        else:
            raise NotImplementedError("C_init method {} not implemented".format(C_init))

        if C_init in ["complex_normal"]:
            self.C = C_init(C_key, (self.H, 2 * self.P, 2))
        else:
            self.C = init_CV(C_init, C_key, (self.H, local_P, 2), V)

        self.D = normal(stddev=1.0)(D_key, (self.H,))

        # Initialize learnable discretisation timescale value
        self.log_step = init_log_steps(step_key, (self.P, dt_min, dt_max))

        self.step_rescale = step_rescale
        self.discretisation = discretisation

    def __call__(self, input_sequence):
        if self.clip_eigs:
            Lambda = jnp.clip(self.Lambda_re, None, -1e-4) + 1j * self.Lambda_im
        else:
            Lambda = self.Lambda_re + 1j * self.Lambda_im

        B_tilde = self.B[..., 0] + 1j * self.B[..., 1]
        C_tilde = self.C[..., 0] + 1j * self.C[..., 1]

        step = self.step_rescale * jnp.exp(self.log_step[:, 0])

        # Discretize
        if self.discretisation in ["zoh"]:
            Lambda_bar, B_bar = discretize_zoh(Lambda, B_tilde, step)
        elif self.discretisation in ["bilinear"]:
            Lambda_bar, B_bar = discretize_bilinear(Lambda, B_tilde, step)
        else:
            raise NotImplementedError(
                "Discretization method {} not implemented".format(self.discretisation)
            )

        ys = apply_ssm(Lambda_bar, B_bar, C_tilde, input_sequence, self.conj_sym)

        # Add feedthrough matrix output Du;
        Du = jax.vmap(lambda u: self.D * u)(input_sequence)
        return ys + Du
    
    def get_activations(self, input_sequence, *, layer_keys: list[str] | str | None = None, return_outputs=False):
        
        activations = {}
        def _capture(k, v):
            if k in ["ssm_output", "ssm_state"]  or layer_keys == "all" or k in layer_keys:
                activations[k] = v
                return
        
        if self.clip_eigs:
            Lambda = jnp.clip(self.Lambda_re, None, -1e-4) + 1j * self.Lambda_im
        else:
            Lambda = self.Lambda_re + 1j * self.Lambda_im

        B_tilde = self.B[..., 0] + 1j * self.B[..., 1]
        C_tilde = self.C[..., 0] + 1j * self.C[..., 1]

        step = self.step_rescale * jnp.exp(self.log_step[:, 0])

        # Discretize
        if self.discretisation in ["zoh"]:
            Lambda_bar, B_bar = discretize_zoh(Lambda, B_tilde, step)
        elif self.discretisation in ["bilinear"]:
            Lambda_bar, B_bar = discretize_bilinear(Lambda, B_tilde, step)
        else:
            raise NotImplementedError(
                "Discretization method {} not implemented".format(self.discretisation)
            )

        Cx, state = apply_ssm_with_activations(Lambda_bar, B_bar, C_tilde, input_sequence, self.conj_sym)
        _capture("ssm_state", state)

        # Add feedthrough matrix output Du;
        Du = jax.vmap(lambda u: self.D * u)(input_sequence)
        output = Cx + Du
        _capture("ssm_output", Cx + Du)
        
        if return_outputs:
            return output, activations
        else:
            return activations


class S5Block(eqx.Module):
    # norm: eqx.nn.BatchNorm
    ssm: S5Layer
    glu: eqx.Module  # Changed from GLU to the more general eqx.Module
    drop: eqx.nn.Dropout 

    def __init__(
        self,
        ssm_size,
        H,
        blocks: int = 1,
        C_init: str = "trunc_standard_normal",
        conj_sym: bool = True,
        clip_eigs: bool = False,
        discretisation: str = "zoh",
        dt_min: float = 0.001,
        dt_max: float = 0.1,
        step_rescale: float = 1.0,
        use_glu: bool = True,  # <-- Add this parameter
        drop_rate=0.05,
        *,
        key
    ):
        ssmkey, glukey = jr.split(key, 2)
        # self.norm = eqx.nn.BatchNorm(
        #     input_size=H, axis_name="batch", channelwise_affine=False, mode="batch"
        # )
        self.ssm = S5Layer(
            ssm_size,
            blocks,
            H,
            C_init,
            conj_sym,
            clip_eigs,
            discretisation,
            dt_min,
            dt_max,
            step_rescale,
            key=ssmkey,
        )

        # Conditionally initialize the GLU or an Identity layer
        if use_glu:
            self.glu = GLU(H, H, key=glukey)
        else:
            self.glu = eqx.nn.Identity()

        self.drop = eqx.nn.Dropout(p=drop_rate)

    # No changes are needed for the __call__ method.
    def __call__(self, x, state, *, key):
        """Compute S5 block."""
        dropkey1, dropkey2 = jr.split(key, 2)
        skip = x
        # x, state = self.norm(x.T, state)
        # x = x.T
        x = self.ssm(x)
        x = jax.nn.gelu(x)
        x = self.drop(x, key=dropkey1)
        x = jax.vmap(self.glu)(x)  # This line works for both GLU and Identity
        x = self.drop(x, key=dropkey2)
        return x, state

    def get_activations(self, x, state, *, layer_keys: list[str] | str | None = None, return_outputs=False):
        """Compute S5 block."""
        
        activations = {}
        def _capture(k, v):
            if k in ["ssm_output", "ssm_state"]  or layer_keys == "all" or k in layer_keys:
                activations[k] = v
                return
        
        # x, state = self.norm(x.T, state)
        # x = x.T
        x, ssm_activations = self.ssm.get_activations(x, return_outputs=True)
        activations.update(ssm_activations)
        post_gelu = jax.nn.gelu(x)
        _capture(f"ssm_post_gelu", post_gelu)
        post_glu = jax.vmap(self.glu)(post_gelu)
        _capture(f"ssm_post_glu", post_glu)
        if return_outputs:
            return post_glu, state, activations
        else:
            return activations

class SSMDownstreamDecoder(eqx.Module):
    encoder: eqx.nn.Linear
    ssm_blocks: List[S5Block]
    decoder: eqx.nn.Linear
    
    encoder_dropout: eqx.nn.Dropout = eqx.field(static=True)
    decoder_dropout: eqx.nn.Dropout = eqx.field(static=True)

    def __init__(
        self,
        input_dim,
        ssm_io_dim,
        ssm_state_dim,
        ssm_num_layers,
        output_dim,
        ssm_init_diag_blocks: int = 4,
        dropout_p: float = 0.1,
        C_init: str = "trunc_standard_normal",
        conj_sym: bool = True,
        clip_eigs: bool = False,
        discretisation: str = "zoh",
        dt_min: float = 0.001,
        dt_max: float = 0.1,
        step_rescale: float = 1.0,
        use_glu: bool = True,
        *,
        key
    ):
        encoder_key, block_key, decoder_key = jr.split(key, 3)

        self.encoder = eqx.nn.Linear(input_dim, ssm_io_dim, key=encoder_key)
        self.encoder_dropout = eqx.nn.Dropout(p=dropout_p)

        block_keys = jr.split(block_key, ssm_num_layers)
        self.ssm_blocks = [
            S5Block(
                ssm_size=int(ssm_io_dim),
                blocks=ssm_init_diag_blocks,
                H=ssm_state_dim,
                C_init=C_init,
                conj_sym=conj_sym,
                clip_eigs=clip_eigs,
                discretisation=discretisation,
                dt_min=dt_min,
                dt_max=dt_max,
                step_rescale=step_rescale,
                use_glu=use_glu,
                drop_rate=dropout_p,
                key=key,
            )
            for key in block_keys
        ]

        self.decoder = eqx.nn.Linear(ssm_io_dim, output_dim, key=decoder_key)
        self.decoder_dropout = eqx.nn.Dropout(p=dropout_p)

    def __call__(self, x, state, key):
        # Project input to SSM dimension
        x = jax.vmap(self.encoder)(x)

        nkeys = 2 + len(self.ssm_blocks)
        key, *dropkeys = jr.split(key, nkeys+1)

        x = self.encoder_dropout(x, key=dropkeys[0])

        for block, k in zip(self.ssm_blocks, dropkeys[1:-1]):
            x, state = block(x, state, key=k)
            
        x = jax.vmap(self.decoder)(x)
        x = self.decoder_dropout(x, key=dropkeys[-1])
        return x, state

    def get_activations(self, x, state, layer_keys: list[str] | str | None = None, return_outputs=False):
        """
        Computes S5 and optionally returns a dictionary of intermediate activations.
        
        Args:
            x: Input tensor
            state: Model state
            key: PRNG key
            layer_keys: List of activation keys to capture. Special case: if "ssm_post_activation"
                        is included, activations for all SSM layers will be captured.
        """
        def _capture(k, v):
            if k in layer_keys or layer_keys == "all" or layer_keys is None:
                activations[k] = v
                return
            
        activations = {}
        layer_keys = layer_keys
        
        x = jax.vmap(self.encoder)(x)
        _capture("post_encoder", x)
        
        for i, block in enumerate(self.ssm_blocks):
            x, state, block_activations = block.get_activations(x, state, layer_keys=layer_keys, return_outputs=True)
            activations.update({f"{k}_{i}": v for k, v in block_activations.items()})
        
        x = jax.vmap(self.decoder)(x)
        _capture("output", x)
        if return_outputs:        
            return x, state, activations
        else:
            return activations
#######


###### Dataset 
class TrainingBatch(TypedDict):
    inputs: Float[Array, "batch timesteps channels"]
    labels: Integer[Array, "batch timesteps channels"]
    
def make_dataset(batch_size: int = 32, rng_seed: int = 0, timesteps: int = 100, in_channels: int = 10, out_channels: int = 2) -> Iterator[TrainingBatch]:
    key = jr.PRNGKey(rng_seed)
    while True:
        key, input_subkey, label_subkey = jr.split(key, 3)
        inputs = jr.uniform(
            input_subkey, minval=0.0, maxval=1.0, shape=(batch_size, timesteps, in_channels)
        )
        labels = jr.uniform(
            label_subkey, minval=0.0, maxval=1.0, shape=(batch_size, timesteps, out_channels)
        )
        yield TrainingBatch(inputs=inputs, labels=labels)
#######


@eqx.filter_jit
@eqx.filter_grad
def loss_fn(
    model: eqx.Module, 
    batch: TrainingBatch,
    state: eqx.nn.State
    ) -> tuple[Float[Array, ""], eqx.nn.State]:
    output, state = jax.vmap(model, in_axes=(0, None, None))(batch["inputs"], state, jr.PRNGKey(0))
    loss = optax.squared_error(
        output, batch["labels"]
    ).mean()
    return loss


model_factory = ModelFactory(
    constructor=SSMDownstreamDecoder, 
    base_kwargs={
        "input_dim": 10,
        "ssm_io_dim": 16,
        "ssm_state_dim": 16,
        "output_dim": 2,
        "ssm_num_layers": 2
        },
    width_kwargs_names=("ssm_io_dim", "ssm_state_dim"),
    )
optimizer_factory = OptimizerFactory(
    optimizer_fn=optax.adam,
    hyperparams={"learning_rate": 1e-3},
)
training_cfg = TrainingConfig(
    model_factory=model_factory, 
    optimizer_factory=optimizer_factory, 
    loss_fn=loss_fn,
    width_multiplier=4.0
)

# Coordinate check
coord_cfg = CoordinateCheckConfig(
    widths = [2**i for i in range(-1, 9)],
    rng_seeds = range(4),
    dataset_factory = lambda: make_dataset(batch_size=128, rng_seed=0, in_channels=10, out_channels=2),
    steps  = 50
)
runner = CoordinateCheckRunner(training_cfg, coord_cfg)
runner.run(output_dir='results/coord_check/s5')
